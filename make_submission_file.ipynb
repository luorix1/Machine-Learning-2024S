{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71789e04-24f7-43b5-b167-371ab0c379d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab users\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/content/drive/{path to project directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d034100-3318-40cc-9625-027a261e0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from data_utils import MLDataset, collate_fn\n",
    "# from modeling import Seq2SeqModel\n",
    "from modeling_challenge import Seq2SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fe8ee-1124-4cf1-818d-47998ef70b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Use 0th GPU for training\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18fee5-3084-440d-b998-a60b2f174d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add or modify your Seq2SeqModel's hyperparameter (keys and values)\n",
    "kwargs = {\n",
    "    'hidden_dim': 256,       # Hidden dimension size for RNN\n",
    "    'nhead': 4,              # Number of attention heads in the Transformer\n",
    "    'dec_layers': 4,         # Number of layers in the Transformer decoder\n",
    "    'dim_feedforward': 1024, # Dimension of feedforward layers in the Transformer\n",
    "    'dropout': 0.2,          # Dropout rate for the Transformer\n",
    "    'enc_layers': 3,         # Number of RNN layers in the encoder\n",
    "    'rnn_dropout': 0.3,      # Dropout rate for the RNN in the encoder\n",
    "    'max_length': 11,        # Maximum length of the sequences\n",
    "    'cnn_settings': {        # Settings for the CustomCNN\n",
    "        'block1_dim': 32,\n",
    "        'block2_dim': 64,\n",
    "        'block3_dim': 128,\n",
    "        'fc_dim': 256,\n",
    "        'model_type': 'VGG'  # Type of CNN ('VGG' or 'ResNet')\n",
    "    },\n",
    "}\n",
    "kwargs_generate = {\n",
    "    # you can add arguments for your model's generate function\n",
    "    \"max_length\": 10\n",
    "}\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7751c02-3674-4616-a348-7e96c66f81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use your own model class and model path\n",
    "model = Seq2SeqModel(num_classes=NUM_CLASSES, **kwargs).to(device)\n",
    "print(model)\n",
    "\n",
    "model_path = './model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffd8f0-96f0-4a62-b88f-06d1a1500cc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "test_ds = MLDataset('data_final/imgs/test', 'data_final/labels/test_dummy.json')\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "state = torch.load(model_path)\n",
    "model.load_state_dict(state[\"model\"])\n",
    "model.eval()\n",
    "\n",
    "ids = []\n",
    "predictions = []\n",
    "for batch_idx, (data, _, lengths) in enumerate(tqdm(test_dl)):       \n",
    "    data = data.to(device) # (B, T, H, W, C)\n",
    "    \n",
    "    # start tokens should be located at the first position of the decoder input\n",
    "    start_tokens = (torch.ones([data.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tok = model.generate(data, lengths, start_tokens, **kwargs_generate) # (B, T)\n",
    "\n",
    "    for i in range(generated_tok.size(0)):\n",
    "        sample_idx = batch_idx * BATCH_SIZE + i + 1\n",
    "        ids.append(sample_idx)\n",
    "        pred = 0\n",
    "        for j, tok in enumerate(generated_tok[i][:lengths[i].int()].tolist()):\n",
    "            pred += tok * math.pow(28, j)\n",
    "        predictions.append(int(pred))\n",
    "\n",
    "sub_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prediction': predictions\n",
    "})\n",
    "\n",
    "sub_df.to_csv('submission.csv', index=False)\n",
    "print(\"Created submission file successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188835c4-ad1b-48b8-935b-2b5f4b44318f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
