{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf11d07-9856-42da-8125-3099f10e0669",
   "metadata": {},
   "source": [
    "## Environment Setting\n",
    "\n",
    "Google drive mount (for Colab users) and package importing.\n",
    "You can optionally work on a transformer part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280812e1-0a56-4cf8-b6a7-7620874d9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab users\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\", force_remount=True)\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/content/drive/{path to project directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b82d731-fc8f-4681-8e8b-614b58e21bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "from data_utils import MLDataset, collate_fn\n",
    "# from modeling_challenge import Seq2SeqTransformerModel\n",
    "from modeling_1 import Seq2SeqTransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78120bd-5408-45b6-9ea1-ec1f080349ca",
   "metadata": {},
   "source": [
    "## (Optional) Sample Visualization\n",
    "\n",
    "You can see actual sample images and correct answers. Additional matplotlib package is needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55336728-8e6c-4a3c-bf3d-b06e676b70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for reference: see actual samples\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "id_to_char = {}\n",
    "alphabets = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "for i, c in enumerate(alphabets):\n",
    "    id_to_char[i + 1] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f4666f-5a79-47aa-94f7-00c85f938460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: hear ([8, 5, 1, 18])\n",
      "Input image sequence:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just for reference: see actual samples\n",
    "idx = 1234\n",
    "sample = np.load(f\"./data_final/imgs/train/{idx}.npy\")\n",
    "with open(\"./data_final/labels/train.json\", \"r\") as f:\n",
    "    sample_target = json.load(f)[str(idx)]\n",
    "\n",
    "tgt_char = \"\"\n",
    "for i in sample_target:\n",
    "    tgt_char += id_to_char[i]\n",
    "\n",
    "\n",
    "print(f\"Answer: {tgt_char} ({sample_target})\")\n",
    "print(\"Input image sequence:\")\n",
    "\n",
    "plt.figure(figsize=(5, len(sample)))\n",
    "for i, img in enumerate(sample):\n",
    "    plt.subplot(1, len(sample), i + 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400da0bd-7c64-4d23-a24b-b0d097d995ae",
   "metadata": {},
   "source": [
    "## Device and seed setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4579ad2-6031-450c-a30c-cbf5b69fbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Use 0th GPU for training\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fix random seed to increase reproducibility\n",
    "# NOTE: Do not modify here!\n",
    "NUM_CLASSES = 26 + 2  # 26 alphabets + 1 padding index + 1 <s> token (start token)\n",
    "\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "\n",
    "def seed_worker(worker_seed):\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6523c-962c-42fb-bbaa-38d8236797eb",
   "metadata": {},
   "source": [
    "## Model loading and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ffaa119-e2d5-4f4e-a663-5578c4f49359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: modify path and batch size for your setting\n",
    "# NOTE: you can apply custom preprocessing to the training data\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = MLDataset(\"data_final/imgs/train\", \"data_final/labels/train.json\")\n",
    "valid_ds = MLDataset(\n",
    "    \"data_final/imgs/valid_normal\", \"data_final/labels/valid_normal.json\"\n",
    ")\n",
    "challenge_ds = MLDataset(\n",
    "    \"data_final/imgs/valid_challenge\", \"data_final/labels/valid_challenge.json\"\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False\n",
    ")\n",
    "challenge_dl = DataLoader(\n",
    "    challenge_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77af1ae4-496f-4cbd-a2dc-9c429d052d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add or modify your Seq2SeqModel's hyperparameter (keys and values)\n",
    "kwargs = {\n",
    "    'hidden_dim': 128,   # Hidden dimension size for Transformer\n",
    "    'num_layers': 3,  # Number of layers in the Transformer\n",
    "    'nhead': 8,  # Number of heads in the multi-head attention\n",
    "    'decoder_dropout': 0.6,  # Dropout rate for the decoder\n",
    "    'cnn_settings': {   # Settings for the CustomCNN\n",
    "        'block1_dim': 32,\n",
    "        'block2_dim': 64,\n",
    "        'block3_dim': 128,\n",
    "        'fc_dim': 128,\n",
    "        'model_type': 'VGG'  # Type of CNN ('VGG' or 'ResNet')\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595dcc8b-006e-4730-94f6-933c2c743996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (cnn): CustomCNN(\n",
      "      (block1): VGGBlock(\n",
      "        (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (block2): VGGBlock(\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (block3): VGGBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (fc): Linear(in_features=1152, out_features=128, bias=True)\n",
      "    )\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.6, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.6, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.6, inplace=False)\n",
      "          (dropout2): Dropout(p=0.6, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embedding): Embedding(28, 128)\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.6, inplace=False)\n",
      "    )\n",
      "    (transformer_decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.6, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.6, inplace=False)\n",
      "          (dropout2): Dropout(p=0.6, inplace=False)\n",
      "          (dropout3): Dropout(p=0.6, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=128, out_features=28, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = Seq2SeqTransformerModel(num_classes=NUM_CLASSES, **kwargs).to(device)\n",
    "model = Seq2SeqTransformerModel(num_classes=NUM_CLASSES, **kwargs).to(device)\n",
    "print(model)\n",
    "##############################################################################\n",
    "#                          IMPLEMENT YOUR CODE                               #\n",
    "##############################################################################\n",
    "model_optim = model_optim = torch.optim.AdamW(model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(model_optim, step_size=10, gamma=0.5)\n",
    "# NOTE: you can define additional components like lr_scheduler, ...\n",
    "##############################################################################\n",
    "#                          END OF YOUR CODE                                  #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc52297-5653-4a17-8ec1-b529bb0e9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: you can freely modify or add training hyperparameters\n",
    "print_interval = 1000\n",
    "max_epoch = 30\n",
    "patience = 2\n",
    "vis = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd18ddf1-a8d0-4ef6-a05f-ac4a38678b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    model_optim,\n",
    "    loss_fn,\n",
    "    max_epoch,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    scheduler,\n",
    "    load_path=None,\n",
    "    save_path=\"./model.pt\",\n",
    "    patience=5\n",
    "):\n",
    "    # Load states if a path is provided\n",
    "    loaded_epoch = 0\n",
    "    loaded_best_valid_loss = -1\n",
    "    if load_path is not None:\n",
    "        state = torch.load(load_path)\n",
    "        model.load_state_dict(state[\"model\"])\n",
    "        model_optim.load_state_dict(state[\"optimizer\"])\n",
    "        loaded_epoch = state[\"epoch\"]\n",
    "        loaded_best_valid_loss = state[\"best_valid_loss\"]\n",
    "\n",
    "    best_valid_loss = float('inf') if loaded_best_valid_loss == -1 else loaded_best_valid_loss\n",
    "\n",
    "    # Early stopping criteria\n",
    "    no_improvement_epochs = 0\n",
    "\n",
    "    # Initialize lists to store loss values\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        step = 0\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target, lengths) in enumerate(tqdm(train_dl)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Prepare the start tokens for the decoder input\n",
    "            start_tokens = (torch.ones([target.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "            decoder_input = torch.cat([start_tokens, target[:, :-1]], dim=1)\n",
    "\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(data, lengths, decoder_input)\n",
    "\n",
    "            # Reshape logits and target for CrossEntropyLoss\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            target = target.view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(logits, target)\n",
    "\n",
    "            # Backward propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model parameters\n",
    "            model_optim.step()\n",
    "\n",
    "            train_loss += loss.detach().cpu().item()\n",
    "\n",
    "        train_loss_avg = train_loss / (batch_idx + 1)\n",
    "        train_losses.append(train_loss_avg)\n",
    "        print(f\"epoch {epoch + 1}, train loss: {train_loss_avg}\")\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        valid_loss = 0\n",
    "        correct_sequences = 0\n",
    "        total_sequences = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch_idx, (data, target, lengths) in enumerate(tqdm(valid_dl)):\n",
    "            with torch.no_grad():\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                # Prepare the start tokens for the decoder input\n",
    "                start_tokens = (torch.ones([target.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "                decoder_input = torch.cat([start_tokens, target[:, :-1]], dim=1)\n",
    "\n",
    "                # Forward pass for validation\n",
    "                logits = model(data, lengths, decoder_input)\n",
    "\n",
    "                # Reshape logits and target for CrossEntropyLoss\n",
    "                logits = logits.view(-1, logits.size(-1))\n",
    "                target = target.view(-1)\n",
    "                loss = loss_fn(logits, target)\n",
    "\n",
    "                # Calculate sequence-level accuracy\n",
    "                logits = logits.view(data.size(0), -1, logits.size(-1))\n",
    "                predicted_sequences = torch.argmax(logits, dim=-1)\n",
    "                for i in range(data.size(0)):\n",
    "                    pred_seq = predicted_sequences[i][: int(lengths[i])]\n",
    "                    target_seq = target.view(data.size(0), -1)[i][: int(lengths[i])]\n",
    "                    if torch.equal(pred_seq, target_seq):\n",
    "                        correct_sequences += 1\n",
    "                    total_sequences += 1\n",
    "\n",
    "                valid_loss += loss.cpu().item()\n",
    "\n",
    "        valid_loss /= (batch_idx + 1)\n",
    "        accuracy = correct_sequences / total_sequences\n",
    "        accuracies.append(accuracy)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(f\"valid epoch: {epoch + 1}, valid loss: {valid_loss:.4f}, accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            print(\"New best valid loss, saving model\")\n",
    "            state = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": model_optim.state_dict(),\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"best_valid_loss\": best_valid_loss,\n",
    "            }\n",
    "            torch.save(state, save_path)\n",
    "            best_valid_loss = valid_loss\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"No improvement in validation loss for {patience} epochs. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, valid_losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb670ac-6d52-458d-b9e8-ded745258c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 30/531 [00:04<01:06,  7.48it/s]"
     ]
    }
   ],
   "source": [
    "load_path = None\n",
    "train_losses, valid_losses, accuracies = train(\n",
    "    model,\n",
    "    model_optim,\n",
    "    loss_fn,\n",
    "    max_epoch,\n",
    "    train_dl,\n",
    "    valid_dl,\n",
    "    scheduler,\n",
    "    load_path=load_path,\n",
    "    save_path=\"./model.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(valid_losses, label=\"valid loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation accuracy over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(accuracies, label='validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83246b-de55-4668-bb78-c1fbb6c7f60d",
   "metadata": {},
   "source": [
    "## Model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3646f-f3a3-4d7f-b71c-9665f11450d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_generate = {\n",
    "    \"max_length\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce38064-e195-46c5-851c-43b116be5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "\n",
    "def eval(dataloader, model_path):\n",
    "    state = torch.load(model_path)\n",
    "    model.load_state_dict(state[\"model\"])\n",
    "    model.eval()\n",
    "\n",
    "    id_to_char = {}\n",
    "    id_to_char[0] = \"<pad>\"\n",
    "    id_to_char[27] = \"<s>\"\n",
    "    alphabets = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for i, c in enumerate(alphabets):\n",
    "        id_to_char[i + 1] = c\n",
    "\n",
    "    results = []\n",
    "    labels = []\n",
    "    for batch_idx, (data, target, lengths) in enumerate(tqdm(dataloader)):\n",
    "        data = data.to(device)  # (B, T, H, W, C)\n",
    "        target = target.to(device)  # (B, T)\n",
    "\n",
    "        # start tokens should be located at the first position of the decoder input\n",
    "        start_tokens = (torch.ones([target.size(0), 1]) * 27).to(torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tok = model.generate(\n",
    "                data, lengths, start_tokens, **kwargs_generate\n",
    "            )  # (B, T)\n",
    "\n",
    "        for i in range(generated_tok.size(0)):\n",
    "            decoded = \"\"\n",
    "            for j in generated_tok[i][: lengths[i].int()].tolist():\n",
    "                decoded += id_to_char[j]\n",
    "            results.append(decoded)\n",
    "\n",
    "            decoded = \"\"\n",
    "            for j in target[i][: lengths[i].int()].tolist():\n",
    "                decoded += id_to_char[j]\n",
    "            labels.append(decoded)\n",
    "\n",
    "    corrects = []\n",
    "    for i in range(len(results)):\n",
    "        if results[i] == labels[i]:\n",
    "            corrects.append(1)\n",
    "        else:\n",
    "            corrects.append(0)\n",
    "    print(\"Accuracy: %.5f\" % (sum(corrects) / len(corrects)))\n",
    "\n",
    "    return results, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce11344-4b75-41a7-9701-9d372f840f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and evaluate your model\n",
    "load_path = \"./model.pt\"\n",
    "print(\"Evaluation with validation set\")\n",
    "results, labels = eval(valid_dl, load_path)\n",
    "\n",
    "print(\"Evaluation with challenge set\")\n",
    "results, labels = eval(challenge_dl, load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0734a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
