{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "from data_utils import MLDataset, collate_fn\n",
    "from modeling_challenge import Seq2SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Use 0th GPU for training\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fix random seed to increase reproducibility\n",
    "# NOTE: Do not modify here!\n",
    "NUM_CLASSES = 26 + 2  # 26 alphabets + 1 padding index + 1 <s> token (start token)\n",
    "\n",
    "random_seed = 7\n",
    "torch.manual_seed(random_seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# %env CUBLAS_WORKSPACE_CONFIG=:16:8\n",
    "\n",
    "\n",
    "def seed_worker(worker_seed):\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "num_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select augmentations to apply\n",
    "augmentations = [\n",
    "    \"random_translate\",\n",
    "    # \"color_jitter\",\n",
    "    # \"random_rotation\",\n",
    "    # \"gaussian_blur\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: modify path and batch size for your setting\n",
    "# NOTE: you can apply custom preprocessing to the training data\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = MLDataset(\"data_final/imgs/train\", \"data_final/labels/train.json\")\n",
    "valid_ds = MLDataset(\n",
    "    \"data_final/imgs/valid_normal\", \"data_final/labels/valid_normal.json\"\n",
    ")\n",
    "challenge_ds = MLDataset(\n",
    "    \"data_final/imgs/valid_challenge\", \"data_final/labels/valid_challenge.json\"\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    valid_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False\n",
    ")\n",
    "challenge_dl = DataLoader(\n",
    "    challenge_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add or modify your Seq2SeqModel's hyperparameter (keys and values)\n",
    "kwargs = {\n",
    "    'hidden_dim': 256,       # Hidden dimension size for RNN\n",
    "    'nhead': 4,              # Number of attention heads in the Transformer\n",
    "    'dec_layers': 4,         # Number of layers in the Transformer decoder\n",
    "    'dim_feedforward': 1024, # Dimension of feedforward layers in the Transformer\n",
    "    'dropout': 0.2,          # Dropout rate for the Transformer\n",
    "    'enc_layers': 4,         # Number of RNN layers in the encoder\n",
    "    'rnn_dropout': 0.2,      # Dropout rate for the RNN in the encoder\n",
    "    'max_length': 11,        # Maximum length of the sequences\n",
    "    'cnn_settings': {        # Settings for the CustomCNN\n",
    "        'block1_dim': 32,\n",
    "        'block2_dim': 64,\n",
    "        'block3_dim': 128,\n",
    "        'fc_dim': 256,\n",
    "        'model_type': 'VGG'  # Type of CNN ('VGG' or 'ResNet')\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqModel(num_classes=NUM_CLASSES, **kwargs).to(device)\n",
    "print(model)\n",
    "##############################################################################\n",
    "#                          IMPLEMENT YOUR CODE                               #\n",
    "##############################################################################\n",
    "model_optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(model_optim, step_size=10, gamma=0.5)\n",
    "##############################################################################\n",
    "#                          END OF YOUR CODE                                  #\n",
    "##############################################################################"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
